{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b75d5ba-23a3-4c10-9705-6db77c30f5dd",
   "metadata": {},
   "source": [
    "## Requirements:\n",
    " pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffe5ee-3084-4e9d-a071-74ba7034274c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d983b1-e9ec-4ff9-9e81-34ffa913b89b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d456b-b217-4dd5-afbf-fe16c43d0943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d5578b2-2647-4fe4-af5c-37da56ed6b88",
   "metadata": {},
   "source": [
    "## Resize Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4e8038-13ee-4760-89ff-3881717e6018",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def sharpen_frame(frame):\n",
    "    # Define a sharpening kernel\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1,  5, -1],\n",
    "                       [0, -1, 0]])\n",
    "    # Apply the sharpening filter\n",
    "    sharpened_frame = cv2.filter2D(frame, -1, kernel)\n",
    "    return sharpened_frame\n",
    "\n",
    "def process_video(input_file, output_file, width, height):\n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(input_file)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 30.0, (width, height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Resize frame\n",
    "        frame_resized = cv2.resize(frame, (width, height))\n",
    "        # Sharpen the resized frame\n",
    "        frame_sharpened = sharpen_frame(frame_resized)\n",
    "        out.write(frame_sharpened)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb65912f-d768-4221-b8e7-4ef1dbe7ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and dimensions\n",
    "width, height = 704, 576\n",
    "process_video(pathResized, pathImprovedPixel, width, height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58418bec-5cc6-4aa7-8823-c496b6515259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754999af-9a0b-491d-8dc8-e934f75fb523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c3099-1438-41a3-940f-0485ae5a4b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "903bb87c-2766-43c3-8df5-ebeae982669d",
   "metadata": {},
   "source": [
    "## increase the number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102e5882-eaa3-4ab9-9781-9c2ab5ff3c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3096a-69e8-43a1-a4ba-a203cf94a849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a1b1fb4-017a-408e-92f4-4e72836ea4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathOriginal = r\"LowQualityVideos\\Original videos\\video1.mp4\"\n",
    "pathResized = r\"LowQualityVideos\\Resized Videos\\video1.mp4\"\n",
    "pathImprovedPixel = r\"LowQualityVideos\\Repixelated Videos\\video1.mp4\"\n",
    "pathImprovedFrame = r\"LowQualityVideos\\FrameImproved Videos\\video1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef5569b3-632a-44a1-8aa7-1c6f3832e6e4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def increase_frame_rate(input_video_path, output_video_path, frame_multiplier=2):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps * frame_multiplier, (width, height))\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Write the original frame\n",
    "        out.write(frame)\n",
    "        # Duplicate frames according to frame_multiplier\n",
    "        for _ in range(frame_multiplier - 1):\n",
    "            out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "427484f9-9b46-4f86-9a12-dbf7bb8edb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "increase_frame_rate(pathResized, pathImprovedFrame, frame_multiplier=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3daeb19-588d-4974-8806-ae9c4fc7267d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d2b83b-d479-419c-aaed-20b09defebf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1af6e29-7f03-4fea-ad54-084b41884f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae56bda-d9d7-45f9-8a70-9f81c251b1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6d6d6-fe5f-4104-86bb-6286704d204b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e2483-213a-479b-ad5f-07858c45535e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae1ed2-659d-4b70-8834-a0fbdadf53de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707cab9a-0708-433f-9746-7ea1566f9f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbd37dc-d1f7-4d07-99c2-0ee2a232a1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9de91c06-c006-420e-87a4-5a23b55e0fd8",
   "metadata": {},
   "source": [
    "## Track Objects using tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14118f2c-4554-4748-bc80-3d624c141802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2259f5-6f57-42e2-826d-9408e7ebdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tracker import *\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9b63175e-f043-48df-8c77-954379a9bcf8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BackgroundThresholdSensitivity\n",
    "# A lower value (e.g., 20) makes the algorithm more sensitive to small changes, meaning it will detect more subtle movements. Conversely, a higher value makes it less sensitive, requiring larger changes to detect motion.\n",
    "\n",
    "# ObjectSizeSensitivity:\n",
    "# ObjectSizeSensitivity is used to filter out contours that are too small to be considered significant objects. This helps in reducing the number of false positives and noise in the object detection process.\n",
    "# if the number of pixels in a group was less than this number it will not consider it as an object\n",
    "# Biger number, less objects\n",
    "detections = []\n",
    "def ObjectTracker(Path, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity  = 20, ObjectSizeSensitivity = 1000, RemoveShadow = True, PlaySpeed = 10):\n",
    "\n",
    "    countofAllObjectsInBusiestTime = 0\n",
    "    countofAllobjectInAllVideo = 0\n",
    "    \n",
    "    tracker = EuclideanDistTracker()\n",
    "    cap = cv2.VideoCapture(Path)\n",
    "    \n",
    "    object_detector = cv2.createBackgroundSubtractorMOG2(history=BackgroundThresholdHistory, varThreshold = BackgroundThresholdSensitivity)\n",
    "    # history:\n",
    "    # This parameter defines the length of the history or the number of previous frames that the algorithm uses to model the background.\n",
    "    # A higher value means the model will consider more frames, which can be useful in environments where the background changes slowly.\n",
    "    # In this case, history=100 means the model will use the last 100 frames to learn and update the background model.\n",
    "    #\n",
    "    # varThreshold:\n",
    "    # This is the variance threshold for the pixel-wise comparison.\n",
    "    # It determines the sensitivity of the algorithm to changes in the pixel values.\n",
    "    # A lower value (e.g., 20) makes the algorithm more sensitive to small changes, meaning it will detect more subtle movements. Conversely, a higher value makes it less sensitive, requiring larger changes to detect motion.\n",
    "    \n",
    "    \n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Morphological kernel\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break \n",
    "\n",
    "            \n",
    "        height, width, _ = frame.shape\n",
    "        roi = frame[30:height-15, 0:width] # you can define the area that you are intrested to\n",
    "\n",
    "        # 1. Object Detection    \n",
    "        mask = object_detector.apply(roi)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)  \n",
    "\n",
    "        #if RemoveShadow:\n",
    "            #_, mask = cv2.threshold(mask, 254,254, cv2.THRESH_BINARY) # Removing Gray colors or shadows from mask\n",
    "        #else:\n",
    "            #_, mask = cv2.threshold(mask, 0,254, cv2.THRESH_BINARY) # changing Gray colors or shadows from mask to white\n",
    "            \n",
    "\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        detections = []\n",
    "        for cnt in contours:\n",
    "    \n",
    "            # Calculate area and remove small elements\n",
    "            area = cv2.contourArea(cnt)\n",
    "\n",
    "            \n",
    "            # ObjectSizeSensitivity:\n",
    "            # ObjectSizeSensitivity is used to filter out contours that are too small to be considered significant objects. This helps in reducing the number of false positives and noise in the object detection process.\n",
    "            # if the number of pixels in a group was less than this number it will not consider it as an object\n",
    "            if area > ObjectSizeSensitivity:\n",
    "                \n",
    "                # Drowing the object boundary\n",
    "                #cv2.drawContours(frame, [cnt], -1, (0,0,255), 2)\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                \n",
    "                #Drowing a box for object\n",
    "                cv2.rectangle(roi, (x,y),(x + w, y + h),(0 ,255, 0),3)\n",
    "                detections.append([x,y,w,h])\n",
    "    \n",
    "        # 2. Object Tracking\n",
    "        boxes_ids = tracker.update(detections)\n",
    "        \n",
    "        for box_id in boxes_ids:\n",
    "            x, y, w, h, id  = box_id\n",
    "            #cv2.putText(frame, str(id) + ':' + str(box_id), (x,y - 15), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0),2) # showing the box cordinate\n",
    "            cv2.putText(roi, str(id) , (x,y - 15), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,0),2)            \n",
    "            cv2.rectangle(roi, (x,y),(x + w, y + h),(0 ,255, 0),3)            \n",
    "\n",
    "            if countofAllobjectInAllVideo < id:\n",
    "                countofAllobjectInAllVideo = id\n",
    "            \n",
    "        if countofAllObjectsInBusiestTime < len(boxes_ids):\n",
    "            countofAllObjectsInBusiestTime = len(boxes_ids)\n",
    "        \n",
    "\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        #cv2.imshow(\"roi\", roi)\n",
    "        cv2.imshow(\"Mask\", mask)\n",
    "\n",
    "        \n",
    "        # PlaySpeed is the speed of play\n",
    "        if cv2.waitKey(PlaySpeed) & 0xFF == 27:\n",
    "            break\n",
    "            \n",
    "\n",
    "    # closing video windoes\n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    return countofAllObjectsInBusiestTime , countofAllobjectInAllVideo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6ae81c8a-5318-40fa-9a8e-094be433e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "PeopleWalking = r\"Videos\\CarinHighway1.mp4\"\n",
    "pathOriginal = r\"LowQualityVideos\\Original videos\\video1.mp4\"\n",
    "pathResized = r\"LowQualityVideos\\Resized Videos\\video1.mp4\"\n",
    "pathImprovedPixel = r\"LowQualityVideos\\Repixelated Videos\\video1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d761ed61-5caa-4cc4-9f53-ae71126ad4a7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[150], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m ObjectTracker(pathOriginal, BackgroundThresholdHistory \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, BackgroundThresholdSensitivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, ObjectSizeSensitivity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m, RemoveShadow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, PlaySpeed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount of object in the busiest frame : \u001b[39m\u001b[38;5;124m\"\u001b[39m , result[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount of All detected object : \u001b[39m\u001b[38;5;124m\"\u001b[39m , result[\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[1;32mIn[146], line 98\u001b[0m, in \u001b[0;36mObjectTracker\u001b[1;34m(Path, BackgroundThresholdHistory, BackgroundThresholdSensitivity, ObjectSizeSensitivity, RemoveShadow, PlaySpeed)\u001b[0m\n\u001b[0;32m     94\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask\u001b[39m\u001b[38;5;124m\"\u001b[39m, mask)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# PlaySpeed is the speed of play\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(PlaySpeed) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# closing video windoes\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "result = ObjectTracker(pathOriginal, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100, RemoveShadow = True, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fa4f5d5e-ab57-4004-9472-ca2b244ce72b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11217f95-a838-44fb-9480-fdcfca979e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d1bfaf-ce49-4be5-bf67-3a3a068bc025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75545d23-c070-4571-8c50-6a77d3805a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b24e2-e383-4fbc-b82d-9dcd1e5992fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982d842e-280f-4791-a3c3-12c32f7fd98c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b36ee-f6b9-41f1-8e8a-9c244b25cd27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba2edc-8f40-4413-8cbc-33256e70222b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0ac20-a76e-4f7d-bda4-bda0ffcce699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca81d83-cb22-4cde-aa50-b801a6ad460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b015ff2-3993-4926-8755-8aa6cbc9c1da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c406f9-1af2-4975-ba48-528b90fb4551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff21d5f7-6724-4cdc-8c44-0fb0f4699165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafe920-4a25-44b4-91f9-6641a51b04b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1c5fe-e469-49b7-b337-44614e271b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "365f805c-5a06-4d3e-b8ac-51d8cf852331",
   "metadata": {},
   "source": [
    "## Object Tracker Using sklearn Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45890b9c-645b-4ab8-966e-664ea87ac90d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "d2ed4033-bbb1-430a-a4b9-c4cb768000da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ec91127-1119-4f8b-af85-e1e5eebb302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from tracker import *\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def ObjectTrackerUsingSklearnCluster(Path, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity=20, ObjectSizeSensitivity=1000, RemoveShadow=True, clustering_eps=20, min_samples=2, PlaySpeed=10):\n",
    "\n",
    "    countofAllObjectsInBusiestTime = 0\n",
    "    countofAllobjectInAllVideo = 0\n",
    "    \n",
    "    tracker = EuclideanDistTracker()  # Initialize your tracker\n",
    "    cap = cv2.VideoCapture(Path)\n",
    "    \n",
    "    object_detector = cv2.createBackgroundSubtractorMOG2(history=BackgroundThresholdHistory, varThreshold=BackgroundThresholdSensitivity)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Morphological kernel\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        height, width, _ = frame.shape\n",
    "        roi = frame[30:height-15, 0:width] # you can define the area that you are intrested to\n",
    "        \n",
    "        mask = object_detector.apply(roi)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        if RemoveShadow:\n",
    "            _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)\n",
    "        else:\n",
    "            _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        detections = []\n",
    "        all_points = []\n",
    "        \n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > ObjectSizeSensitivity:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                detections.append([x, y, w, h])\n",
    "                \n",
    "                # Add all contour points to a list for clustering\n",
    "                for point in cnt:\n",
    "                    all_points.append(point[0])  # Flatten the point structure\n",
    "\n",
    "        # Cluster points based on proximity\n",
    "        if len(all_points) > 0:\n",
    "            clustering = DBSCAN(eps=clustering_eps, min_samples=min_samples).fit(all_points)\n",
    "            labels = clustering.labels_\n",
    "            \n",
    "            unique_labels = set(labels)\n",
    "            for label in unique_labels:\n",
    "                if label == -1:\n",
    "                    continue\n",
    "                \n",
    "                label_points = np.array([all_points[i] for i in range(len(all_points)) if labels[i] == label])\n",
    "                \n",
    "                x, y, w, h = cv2.boundingRect(label_points)\n",
    "                cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "                detections.append([x, y, w, h])\n",
    "        \n",
    "        boxes_ids = tracker.update(detections)\n",
    "        \n",
    "        for box_id in boxes_ids:\n",
    "            x, y, w, h, id = box_id\n",
    "            cv2.putText(roi, str(id), (x, y - 15), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 2)            \n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 3)            \n",
    "\n",
    "            if countofAllobjectInAllVideo < id:\n",
    "                countofAllobjectInAllVideo = id\n",
    "            \n",
    "        if countofAllObjectsInBusiestTime < len(boxes_ids):\n",
    "            countofAllObjectsInBusiestTime = len(boxes_ids)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        #cv2.imshow(\"roi\", roi)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        cv2.imshow(\"Mask\", mask)\n",
    "    \n",
    "        if cv2.waitKey(PlaySpeed) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "\n",
    "    return countofAllObjectsInBusiestTime, countofAllobjectInAllVideo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a127c46-4b09-4cbe-b2c6-8b099e9a37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  4\n",
      "Count of All detected object :  18\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSklearnCluster(pathOriginal, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100, RemoveShadow = True, clustering_eps=50, min_samples=50, PlaySpeed = 50)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39cd83-4eed-4c7c-9d8c-dec8c1823826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79662e79-033d-4676-b331-2ddec51b0580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3c2d1-ef56-4928-9f56-377d9bd6db11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb9c0b1-7a11-4de1-9025-ec0c563cfa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c6b77-b1bd-4244-b69d-76704ef3e31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd6711-538d-4429-8557-7089cc680956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981bab09-597c-4511-8aaf-311f11f2b440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8e69c-f673-493f-9bed-bf442cee7bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e2151f-fb0e-4b6e-8132-e6ba0ca13540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b28d64-ab68-4806-be75-a5f3fa296897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a00cb6ee-99cd-45c0-a3ea-29b374ced9e1",
   "metadata": {},
   "source": [
    "## Object Tracker Using Self Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16fcbf42-e746-4231-a428-ab760e96f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tracker import *\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9f601ba-a5e5-4bb4-a52c-5b575b130970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cluster_contours(detections, distance_threshold=200):\n",
    "    clusters = []\n",
    "    for rect in detections:\n",
    "        x, y, w, h = rect\n",
    "        added_to_cluster = False\n",
    "        for cluster in clusters:\n",
    "            for cx, cy, cw, ch in cluster:\n",
    "                if abs(x - cx) < distance_threshold and abs(y - cy) < distance_threshold:\n",
    "                    cluster.append(rect)\n",
    "                    added_to_cluster = True\n",
    "                    break\n",
    "            if added_to_cluster:\n",
    "                break\n",
    "        if not added_to_cluster:\n",
    "            clusters.append([rect])\n",
    "    return clusters\n",
    "\n",
    "def ObjectTrackerUsingSelfCluster(Path, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity=20, ObjectSizeSensitivity=1000, ObjectsDistanceTreshhold = 200, RemoveShadow=True, PlaySpeed=10):\n",
    "\n",
    "    countofAllObjectsInBusiestTime = 0\n",
    "    countofAllobjectInAllVideo = 0\n",
    "    \n",
    "    tracker = EuclideanDistTracker()\n",
    "    cap = cv2.VideoCapture(Path)\n",
    "    \n",
    "    object_detector = cv2.createBackgroundSubtractorMOG2(history=BackgroundThresholdHistory, varThreshold=BackgroundThresholdSensitivity)\n",
    "    \n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))  # Morphological kernel\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        height, width, _ = frame.shape\n",
    "        roi = frame[30:height-15, 0:width] # you can define the area that you are intrested to\n",
    "\n",
    "        \n",
    "        mask = object_detector.apply(roi)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        if RemoveShadow:\n",
    "            _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)\n",
    "        else:\n",
    "            _, mask = cv2.threshold(mask, 0, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        detections = []\n",
    "        \n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area > ObjectSizeSensitivity:\n",
    "                x, y, w, h = cv2.boundingRect(cnt)\n",
    "                detections.append([x, y, w, h])\n",
    "\n",
    "        clusters = cluster_contours(detections,ObjectsDistanceTreshhold)\n",
    "        \n",
    "        combined_detections = []\n",
    "        for cluster in clusters:\n",
    "            if len(cluster) > 1:\n",
    "                cluster_x = min([x for x, y, w, h in cluster])\n",
    "                cluster_y = min([y for x, y, w, h in cluster])\n",
    "                cluster_w = max([x + w for x, y, w, h in cluster]) - cluster_x\n",
    "                cluster_h = max([y + h for x, y, w, h in cluster]) - cluster_y\n",
    "                combined_detections.append([cluster_x, cluster_y, cluster_w, cluster_h])\n",
    "            else:\n",
    "                combined_detections.append(cluster[0])\n",
    "        \n",
    "        boxes_ids = tracker.update(combined_detections)\n",
    "        \n",
    "        for box_id in boxes_ids:\n",
    "            x, y, w, h, id  = box_id\n",
    "            cv2.putText(roi, str(id), (x, y - 15), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 2)            \n",
    "            cv2.rectangle(roi, (x, y), (x + w, y + h), (0, 255, 0), 3)\n",
    "\n",
    "            if countofAllobjectInAllVideo < id:\n",
    "                countofAllobjectInAllVideo = id\n",
    "            \n",
    "        if countofAllObjectsInBusiestTime < len(boxes_ids):\n",
    "            countofAllObjectsInBusiestTime = len(boxes_ids)\n",
    "        \n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "        #cv2.imshow(\"roi\", roi)\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        cv2.imshow(\"Mask\", mask)\n",
    "    \n",
    "        if cv2.waitKey(PlaySpeed) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    #cv2.destroyAllWindows()  \n",
    "\n",
    "    return countofAllObjectsInBusiestTime, countofAllobjectInAllVideo\n",
    "\n",
    "# Example usage\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b2887a2-60fd-417e-aba6-d6d4637512fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  6\n",
      "Count of All detected object :  30\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSelfCluster(pathOriginal, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100,ObjectsDistanceTreshhold = 5, RemoveShadow = False, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b5637-1aa0-44d2-8f52-70cda1c73357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a7ce10-b972-4a1c-ae38-48da796b6c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c498e-0409-41af-8aee-6cd72e597547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0d117-77ae-4701-8d85-5ea7d294cc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b128401e-f11b-44dc-9e05-3f6912251d06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd4a151-74e6-468d-b5ad-d384d5f9417d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3bed3-fbd8-45b8-a25e-d77f88bdebfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54376067-3a55-403e-b8fd-186f3d3e4d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f15dd0d-fc50-4907-8b93-9877c0967ba9",
   "metadata": {},
   "source": [
    "# Executing Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3926d1-948c-4330-b6f9-bcfa31559054",
   "metadata": {},
   "source": [
    "## Low Quality Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdc8efa4-bbf5-49a5-b7b5-a4d33cd6af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathOriginal = r\"LowQualityVideos\\Original videos\\video3.mp4\"\n",
    "# pathResized = r\"LowQualityVideos\\Resized Videos\\video1.mp4\"\n",
    "# pathImprovedPixel = r\"LowQualityVideos\\Repixelated Videos\\video1.mp4\"\n",
    "# pathImprovedFrame = r\"LowQualityVideos\\FrameImproved Videos\\video1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "661e33b4-28f0-4e57-8411-10fae0e7d1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  3\n",
      "Count of All detected object :  7\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTracker(pathOriginal, BackgroundThresholdHistory = 50, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100, RemoveShadow = True, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "648805ce-d3e2-40cb-b8d7-6639b7261421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  5\n",
      "Count of All detected object :  41\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSklearnCluster(pathOriginal, BackgroundThresholdHistory = 100, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100, RemoveShadow = True, clustering_eps=50, min_samples=50, PlaySpeed = 50)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0847f99-2787-46aa-8f59-9c1531640837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  9\n",
      "Count of All detected object :  91\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSelfCluster(pathOriginal, BackgroundThresholdHistory = 100, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 100,ObjectsDistanceTreshhold = 10, RemoveShadow = False, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d80fa-2a6b-4d10-bf2a-6736b1c85c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eac2254-0492-4000-ab89-70c938a98c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc5a56ec-02eb-46eb-874b-6d64df75d271",
   "metadata": {},
   "source": [
    "## NormalVideos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "3b218ba8-853a-4775-9e7f-fdc3062dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathOriginal = r\"Videos\\CarinHighway1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9a690e71-256b-442b-8e56-3530ba6f63a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  2\n",
      "Count of All detected object :  34\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTracker(pathOriginal, BackgroundThresholdHistory = 100, BackgroundThresholdSensitivity = 500, ObjectSizeSensitivity = 10000, RemoveShadow = True, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d789ab6f-f56e-4393-8d0a-2af0d98857c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  7\n",
      "Count of All detected object :  94\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSklearnCluster(pathOriginal, BackgroundThresholdHistory = 100, BackgroundThresholdSensitivity = 1500, ObjectSizeSensitivity = 500, clustering_eps=500, min_samples=1000, PlaySpeed = 50)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "54a30841-2e12-402d-a140-d159747622c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  7\n",
      "Count of All detected object :  90\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTrackerUsingSelfCluster(pathOriginal, BackgroundThresholdHistory = 100, BackgroundThresholdSensitivity = 1500, ObjectSizeSensitivity = 500,ObjectsDistanceTreshhold = 50, RemoveShadow = False, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d559810c-585b-4468-8039-c92c281ca3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3cc50a-3c9a-42a7-9187-e3d06c2ea5a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "55af9608-48bf-492e-9c99-60836b922167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of object in the busiest frame :  3\n",
      "Count of All detected object :  91\n"
     ]
    }
   ],
   "source": [
    "result = ObjectTracker(pathOriginal, BackgroundThresholdHistory = 1000, BackgroundThresholdSensitivity = 100, ObjectSizeSensitivity = 10000, RemoveShadow = True, PlaySpeed = 30)\n",
    "print( \"Count of object in the busiest frame : \" , result[0])\n",
    "print( \"Count of All detected object : \" , result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dad858f-c6d1-41d2-8ce7-ef52754f9e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6efc76-311b-4056-b764-2bd63359bc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
